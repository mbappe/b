Run makewrap to see if the builds work.  It will run for days (maybe not).
Run bbwrap to see if the code works.  It will run for days (maybe not).
Run bbwrap -q for a less vigorous test.  It will run for days (maybe not).
rcs.tjz contains a copy of the RCS files in case the originals be found.
Look for symbolic names in the RCS files.
CP140405 is Checkpoint April 5, 2014.

For 64-bit, bpd8, dab2, lsmax16, nobm, (gcc or clang):
 -  best: ppsw
 - worst: ppln (not much different; full spread is 43-46)
For 32-bit, bpd8, dab2, (gcc or clang):
 -  best: ppsw
 - worst: ppln (10% worse; 25-27))
For 64-bit, bpd6, dab3, lsmax16, nobm, (gcc or clang):
 -  best: ppsw
 - worst: ppln (50% worse; 48.5 - 74.5))
For 32-bit, bpd6, dab3, (gcc or clang):
 -  best: ppsw
 - worst: ppln (10% worse; 35.5 - 39.5)
For 64-bit, bpd4, dab4, lsmax16, nobm, (gcc or clang):
 -  best: ppsw
 - worst: ppln (10% worse; 85.5 - 94.5)
For 32-bit, bpd4, dab4, lsmax16, nobm, (gcc or clang):
 -  best: ppsw
 - worst: ppln (10% worse; 41 - 44)
For 64-bit, bpd3, dab5, lsmax16, nobm, (gcc or clang):
 -  best: ppsw (noskipln is a little better)
 - worst: ppln (10% ish)
For 32-bit, bpd3, dab5, lsmax16, nobm, (gcc or clang):
 -  best: ppsw
 - worst: ppln (12% worse; 50 - 56) (skipln slightly better than other two)

For 64-bit, bpd3, dab5, lsmax16, (gcc or clang): nobm-bmsw: 82-118
For 64-bit, bpd8, dab2, lsmax16, (gcc or clang): nobm-bmsw: 44-92
For 64-bit, bpd6, dab3, lsmax16, (gcc or clang): nobm-bmsw: 49-88

Below follows partially thought-out ideas.

5/24/14:

Decode the relatively small type field encoded in the low bits of the
link pointer with the help of a context.  The context can be inferred
from the traversal down the tree.  A simple example of context is the
number of switches traversed so far.  But we could add context from
the population and/or by explicitly including it in nodes that have
decode information in the first word.

The ultimate performance (until totally full population) is a bitmap
that spans the expanse.  We can't use this from the outset because it
would use too much memory.

A binary tree to a one-word bitmap leaf uses too much memory and has
too many levels.

Stick with the one-word bitmap.

First insert creates skip link to one word bitmap leaf with one bit set.
How do we know it is a skip link to a one word bitmap leaf?
If we need an extra word for it, then it's two words for the first key.
Second insert creates a one bit switch (if necessary) at the appropriate
level.  Now using seven words.  Yuck.
Third insert creates another switch and now using 11 words.

First insert creates a list using two words (one word for key and one
word of malloc overhead) (requires the use of one precious type value;
at least one type value at the top).
Second insert makes it four words (one word of length plus two words
of keys plus one word of malloc overhead).
Third insert makes it six words (one word of length plus three words
of keys plus two words of malloc overhead).
Fourth insert is still six words.
Fifth insert is eight words.
Seventh insert is ten words.
Ninth insert creates a binary switch makes it 16 or 18 words (three words
for switch plus one word malloc overhead plus two lists with 12 or 14 words).
We could put the switch and the lists in the same chunk of memory, e.g.
(type, len0, len1, key00, key01, ... , key10, key11, key12, ...).
How big can this (type, len0, ... lenN-1, keys) get?  Concern is insert
times?

5/24/14:

Representation:

There is one line per non-NULL link.
The number in the line is the sub-key of the link (it might not be the
same as the offset if we have a compressed switch or the link
is a skip link.
Dot means ditto, i.e. bit is the same as in the previous line.
Do we distinguish an uncompressed switch explicitly?
Do we distinguish a skip link explicitly?
Do we support non-NULL links with zero population?

We use colons after the sub-key to indicate how many bits are decoded by
the switch pointed to.
If the sub-key in the next line extends beyond the colons then it is a
skip link.

0:                // Link to a 1-bit switch.
.0::::            // Link to a 4-bit switch.
..1001::          // Link to a 2-bit switch.
......010101::    // Skip link to a 2-bit switch.
............00::  // Link to a 2-bit switch at the bottom.
..............01  // Link to leaf.
..............11
............10::
..............01
..............11
............11::
..............10
......11:::       // Link to a 3-bit switch.
........01111001  // Skip link to leaf.
1001::::::::::::  // Skip link to a 12-bit switch at the bottom.
....101010101101  // Link to leaf.
....101011000100

5/21/2014:
- Bitmap leaf.
- Uncompressed switch.
- List leaf.
- Bitmap switch.
type 0: Look at first word of object pointed to for additional type info.
type N: Uncompressed switch at f(N, nDL) digits left.
type C: Bitmap switch at next digit down.

5/16/2014:
Use type==0 for anything other than uncompressed switch with one-word links.

5/15/2014:

When list gets too long create a two or four way switch.  Or add one more
bit to the parent switch.  Where do we keep switch width/depth info?  At
beginning of switch?

==============

First 63 keys go into double cache line list (128 bytes).
What happens when the 64th key is inserted?
64 keys supports 512 bytes.
How do I insert 64 keys in the 512 bytes supported by them and how do we
index them?
Divide 64 keys into eighths?  firstkey[0..7] gives first key in each eighth.
Put one eighth of keys in each eighth of 512 bytes.
Insert maintains firstkey[0..7] and moves keys to balance if/when necessary.
Is firstkey in switch or at beginning of buf?
What happens when 256th key is inserted?
256 keys support 2K bytes.
How do I insert 256 keys into the 2K bytes supported by them and how do we
index them?
Divide into 32?  firstkey[0..31]?  One single cache line of first key.
One single cache line of keys per search.
Firstkey is at beginning of buf?
What happens with 1Kth key is inserted?  Convert to bitmap.
Convert to bitmap on 1024th key for one word  per key.
Convert to bitmap on  512th key for two words per key.

5/13/2014:

One bit for each cache line that has a key.
One cache line for each key.
Each cache line is 128 bytes which is 16 words.  Too much.

The partition with the most keys must fit in one cache line.
Can other cache lines represent multiple partitions?

The bitmap is 8K bytes which is 64 double cache lines or 128 single
cache lines.
No need for our lists to handle more than 128 single cache lines.
Let's say we can grow to 127 single cache lines before converting to bitmap.
What do the cache lines hold and how do we represent what they hold?
Each single cache line could be a list of keys.
How do we know which line the key we're looking for is in?
7-bits picks a line.  9-bits of key left.  7 keys per word.  56 keys per line.
128 keys using 8K bytes = 8 words per key.
We can't dedicate a whole single cache line to one key.
We must pick the correct line for each key.

5/13/2014:

First notes after experiencing the benefits of malloc using huge pages.
The encoded type field in the least significant bits of the root word could
be used to index into a table of decoded types (in the way Doug
uses one big switch statement to decode a larger type field).  We could also
use the current depth to index into the table for free with a one word link.
A combination of the two indexes could give us number-of-types times
number-of-depths possible decoded types.  But the decoded type at any point
in the tree can't really be dynamic because it doesn't depend on anything
specific about this particular tree.  In order to improve adaptibility we
have to include the population in the index into the decode of the type.
In order for that to be free in a one-word link we have to include the
population in the least significant bits of the root word -- or -- not
necessarily the population, but we have to change the encoded type based
on the population.  We have 2^lsbits * depth to tell us what we are pointing
at.  Or another way to look at it is we have 2^lsbits at every depth to
tell us what we're pointing at.  But, at large depths we have to represent
all possible smaller depths which takes away a bunch of our 2^lsbits values.
Unless we restrict the smaller depths we choose to allow.
We want to avoid extra memory references when out of the cache.  They
shouldn't matter as much when in the cache.  So we want to restrict lists
that are out of the cache to a single cache line (64 or 128 bytes) and that
line must include the list header.
And we want to make sure that the switch above a bitmap is in the cache.
I think it implies the lowest level switch is at the same level whether the
leaf is a bitmap or a list.
For 64-bit pointers and a 32-bit expanse, leaving 12-bits to decode below
the lowest level switch means 1 million links at the next level up.  That's
8 million bytes with one word links and 16 million bytes with two word links.
The second word doubles our cache requirements or makes our cache half as
effective.  What if we tailor the type table based on how much cache we want
the array to use?  Less cache means more bits to decode at the leaf.
Does it mean more memory used because we transition to bitmap sooner with
smaller population?  Yes.  Exciting.
A 16-bit bitmap means 16-bit keys.  31 keys in one cache line.  63 keys in
two cache lines.  64 keys uses 8K bytes.  That's 16 words per key.  Yikes.
How do we improve memory usage?  Decode more bits in the switch.  But do
it with an implied offset into a contiguous list of lists.
64 keys becomes 128 words which is eight cache lines.  3-bits of decode picks
the cache line leaving 13 bits of key and still only 4 keys per word.
So still only 63 keys in one line pushes us to the next level.  Worst case
is ugly.  We need population based division of the list.  To pick the cache
line.  But isn't insert going to be expensive and involve numerous cache
lines?

4/11/2014:

With 6-bit switches we need 11 type values to decode a full 64-bit key.
If we put a matrix leaf at 16 bits we need to decode only 48 bits before
the leaf and that takes only 8 type values.

A matrix leaf must be reconfigured when we don't have space for a key being
added in the current row and the following key row id is not empty.
Or when inserting a row would increase the size of the leaf to more than
4K - 1 words.
How do we choose the best next configuration?  Increase the size of the key
row id such that the key row id with the most keys just barely fits in a row? 
What configuration do we start with?
A one-word bitmap decodes a 6-bit row id leaving a 10-bit key tail and six
keys per word.  The description indicates the number of words of bitmap.
One word is 6-bits of row id.  Two words of bitmap is 7-bits of row id.
Four words of bitmap is 8-bits of row id.
What is the smallest matrix leaf?  One word of description, One word of
bitmap, and one word of keys.

type: 0 => one word leaf
type: 1 => three word leaf (third word is zero means two keys)
type: 2 => five word leaf (fifth word is zero means four keys)
type: 3 => matrix leaf, one-word bitmap, if all six keys have the same
           high six bits, then we have 5 continuation rows
When do we add a switch?
Use a mask to indicate which bits from the key form the index into the
matrix leaf bitmap?
Use a lookup table to specify the bits per each digit.
Digit 0: 16 bits
Digit 1: 6 bits
Digit 2: 6 bits
Digit 3: 6 bits


4/8/2014:

======================
One-word link scheme:
--------------------
All node types that require a header should share the same value from
the 16 possible values of the low order bits of the pointer to the
node.  The other 15 possible values should be carefully used for types
that do not need a header, e.g. an uncompressed switch at the next level,
an uncompressed switch at a lower level if we're not going to check the
prefix during lookup, a bitmap leaf at the next level.
X => node with header (compressed switch or list leaf or matrix leaf at
     any level, skip to uncompressed switch or bitmap)
Y => bitmap leaf at next level down
Z => uncompressed switch at next level down
{ 0 - 15 } - { X, Y, Z } => uncompressed switch at level(type)
========================
Multi-word link scheme N:
------------------------
Put some additional indexing info in the extra word(s) of the N-word link
to help locate the relevant section of the next node, e.g. bitmap for
compressed switch or matrix leaf, sub-expanse index for list leaf, and/or
put some info that otherwise allows us to avoid looking at a discontiguous
header, e.g. target level and prefix for skip, and/or put immediate
key-tails in the link.
========================
Mixed-word link scheme 2:
------------------------
In addition to the X, Y and Z types from the one-word link scheme add a
type, W, for uncompressed two-word switch and put the rest of the type
info in the lower level link.
{ 0 - 15 } - { W, X, Y, Z } => uncompressed one-word switch at next level
========================
Mixed-word link scheme N:
------------------------
In addition to the X, Y and Z types from the one-word link scheme add a
type, W, for uncompressed N-word switch and put the rest of the type
info in the lower level link.
{ 0 - 15 } - { W, X, Y, Z } => uncompressed N-word switch at next level
========================
Mixed-word link scheme A:
------------------------
Divide the uncompressed switch at next level types from the one-word
link scheme in half and use half for uncompressed one-word link switch
at the next level and the other half for uncompressed two-word link
switch at the next level.
We get half as many levels as the one-word link scheme.
======================

Only one of the 16 values representable by the 4-bit type field in a
pointer should be used for all types that require a discontiguous header.
The other 15 values should be carefully assigned to allow high-performance
traversal of the DAG.  If we don't check the prefix at every level during
lookup, then it makes sense to encode the number of bits skipped in those
values.  If we do check the prefix at every level, then we can't fit
everything we need into those values so we might as well treat the next
switch as an object with a header and not give it a special value.
uncompressed branch.  If we check the prefix at every level then a skip
might as well put the level in the same place.

4/8/2014:

The New Node:
Summing the bits set in a virtual row id bitmap in the header of a node
gives the physical row number of the start of a virtual row (if the
virtual row id has any physical row(s)).  There is a continuation bit
(possibly implied) in the first word of each physical row indicating
whether or not the physical row is the beginning of a virtual row.
If the continuation bit is set, then the physical row is a continuation
of the virtual row from the previous physical row and it is not the
beginning of a new virtual row.  It means the virtual row id has no
physical presence and the virtual row id bit was set in the virtual row
id bitmap only so that subsequent virtual rows can be located correctly.

A row can contain key-tails.  A row can contain (sub-key, pointer-to-node)
pairs.  Can a row contain both?  Can a row contain (sub-key, key-tails)?

A bitmap leaf can be any size because there is only one dereference and it
will probably be a TLB and a data cache miss.
What about switches?  Does it make sense to group node headers for
multiple nodes into a single page?

root word -> top switch header -> top switch pointer -> next switch header
          -> next switch pointer -> leaf (header and data)

Switch headers are collected in switch header pages.

root-word -> bitmap-descriptor(key, nBL) -> bitmap-bit
root-word -> switch-descriptor(key, nBL) -> switch-entry

4/7/2014: Are there benefits to using half-word and quarter-word local
variables?  Can it make the code faster by giving the compiler more registers
to work with?

4/6/2014: The New (64-bit) Leaf:
First word is prefix up and bits left?
Type tells how many bits of prefix down, e.g. size of bitmap index.
Bitmap locates start of list of keys for each prefix, if present.
First word of row contains prefix -- may have prefix mismatch.
First word also contains length of row in keys.
Words contain integral number of keys for parallel search.
BL=64,   1 Row  x n Words =   n     64-bit Keys (minus bits for prefix)
...
BL=64, 2^m Rows x n Words =   n*2^m 64-bit Keys (minus bits for prefix)
BL=32,   1 Row  x n Words =  2n     32-bit Keys (minus bits for prefix)
...
BL=32, 2^m Rows x n Words =  2n*2^m 32-bit Keys (minus bits for prefix)
BL=21,   1 Row  x n Words =  3n     21-bit Keys
...
BL=21, 2^m Rows x n Words =  3n*2^m 21-bit Keys

****
BL=16,   1 Row  x n Words =  4n     16-bit Keys
...
BL=16,  64 Rows x n Words =  4n*64  16-bit Keys **** start here ****
...
BL=16, 2^m Rows x n Words =  4n*2^m 16-bit Keys
...
BL=16,  1K Rows x 1 Word  =  Bitmap
****

BL=12,   1 Row  x n Words =  5n     12-bit Keys
...
BL=12, 2^m Row  x n Words =  5n*2^m 12-bit Keys
BL=10,   1 Row  x n Words =  6n     10-bit Keys
...
BL=10, 2^m Rows x n Words =  6n*2^m 10-bit Keys
BL= 9,   1 Row  x n Words =  7n      9-bit Keys
...
BL= 9, 2^m Rows x n Words =  7n*2^m  9-bit Keys
BL= 8,   1 Row  x n Words =  8n      8-bit Keys
...
BL= 8, 2^m Rows x n Words =  8n*2^m  8-bit Keys
BL= 7,   1 Row  x n Words =  9n      7-bit Keys
BL= 6,   1 Row  x 1 Word  = Bitmap
BL= 7,   2 Rows x 1 Word  = Bitmap 
BL= 8,   4 Rows x 1 Word  = Bitmap 
BL= 9,   8 Rows x 1 Word  = Bitmap 
BL=10,  16 Rows x 1 Word  = Bitmap 
BL=11,  32 Rows x 1 Word  = Bitmap 
BL=12,  64 Rows x 1 Word  = Bitmap 
BL=13, 128 Rows x 1 Word  = Bitmap 
BL=14, 256 Rows x 1 Word  = Bitmap 
BL=15, 512 Rows x 1 Word  = Bitmap 
BL=16,  1K Rows x 1 Word  = Bitmap 
BL=17,  2K Rows x 1 Word  = Bitmap 
BL=18,  4K Rows x 1 Word  = Bitmap 
BL=19,  8K Rows x 1 Word  = Bitmap 
BL=20, 16K Rows x 1 Word  = Bitmap 

Q: Is it desirable/possible to coalesce in small steps?  That is, all rows
   are adjacent to start?
Q: When do we create a leaf?  What kind of leaf?
   


Questions:
- Which is faster to search?  Which is more memory efficient?
  A long list of 16-bit keys.  A 64K bit bitmap is 1K words.  A 1K word list
  contains 4K keys.  That's a long list.
  Several lists of 8-bit keys, each with an 8-bit prefix.
  Do we want the lists contiguous?  Or can we use a separate pointer for each?
  A bitmap with 256 bits is only four 64-bit words.  A pointer is one word.
  Three words of list is one 8-bit prefix and 23 keys.
- Does it make sense to have level specific behavior?  Or is it possible
  to maximize performance while minimizing level specific behavior?
- Is it better to do an unconditional prefix check or to test to see if a
  prefix check is necessary before doing the prefix check?  If the test
  to see is just as expensive as the check itself, then it doesn't make
  sense to do the test.  The prefix is a little more computation, but is
  it any more conditional branches?  The prefix check requires reading the
  prefix/pop word which may be an additional cache line if it is not in
  the link.

To do:
- Parallel search.
- Level-specific bits per digit.
- Use type values greater than current level for other things.
- Use ls_nDigitsLeft/ls_wPrefix to implement skip link to list.
- Allow lists at cnDigitsAtBottom.

... Old ...

Mike's high performance binary tree.

Start with a basic binary tree and use compression techniques to improve
memory usage and performance.

- use immediate values, adjacent memory and/or various sized short pointers
  to replace word-sized/long pointers
- discard unnecessary sub-key bits
- widen nodes from single bit to more
- pull node parameters into pointer above
- use bitmaps at bottom
- discard full bitmaps
- use population count to trigger some transformations, e.g. node merging
- use lists instead of uncompressed wide nodes/leaves
- use bitmapped nodes for wide nodes/leaves


